{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "\n",
    "# scikit-hep\n",
    "import awkward as ak\n",
    "import uproot\n",
    "import vector\n",
    "vector.register_awkward()\n",
    "import correctionlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from NTuples without materialising in memory\n",
    "\n",
    "!! Install `xrootd` via `conda-forge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"signal\": \"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/ZprimeToTT_M2000_W20_TuneCP2_PSweights_13TeV-madgraph-pythiaMLM-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/22BAB5D2-9E3F-E440-AB30-AE6DBFDF6C83.root\",\n",
    "    \"tt_semilep\": \"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToSemiLeptonic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/120000/08FCB2ED-176B-064B-85AB-37B898773B98.root\",\n",
    "    \"tt_had\": \"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTToHadronic_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/130000/009086DB-1E42-7545-9A35-1433EC89D04B.root\",\n",
    "    \"tt_lep\": \"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/TTTo2L2Nu_TuneCP5_13TeV-powheg-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/120000/4A9AFE65-CC58-044A-AFBB-F4CAEA0A7FCD.root\",\n",
    "    \"Wjets\": \"root://eospublic.cern.ch//eos/opendata/cms/mc/RunIISummer20UL16NanoAODv9/WJetsToLNu_TuneCP5_13TeV-madgraphMLM-pythia8/NANOAODSIM/106X_mcRun2_asymptotic_v17-v1/270000/00702195-E707-3743-8BBA-57EB9DEE1DBA.root\",\n",
    "    \"data\": \"root://eospublic.cern.ch//eos/opendata/cms/Run2016H/SingleMuon/NANOAOD/UL2016_MiniAODv2_NanoAODv9-v1/130000/0DEE1709-0416-F24B-ACB2-C68997CB6465.root\"\n",
    "}\n",
    "\n",
    "events = defaultdict(dict)\n",
    "for dataset, filename in datasets.items():\n",
    "    print(f\"Opening {dataset} file: {filename}\")\n",
    "    # Open the file\n",
    "    with uproot.open(filename) as f:\n",
    "        f = uproot.open(filename)\n",
    "        events[dataset]= {\"events\": f['Events']}\n",
    "        nevents = events[dataset][\"events\"].num_entries\n",
    "        events[dataset].update({\"nevents\": nevents})\n",
    "        print(f\"{dataset}: {nevents = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read branches into memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, data in events.items():\n",
    "    dataset_events = data[\"events\"]\n",
    "    print(f\"Processing {dataset} dataset\")\n",
    "    # Load the branches we need\n",
    "    events[dataset][\"muon_pt\"] = dataset_events[\"Muon_pt\"].array()\n",
    "    events[dataset][\"muon_eta\"] = dataset_events[\"Muon_eta\"].array()\n",
    "    events[dataset][\"muon_phi\"] = dataset_events[\"Muon_phi\"].array()\n",
    "    events[dataset][\"muon_mass\"] = dataset_events[\"Muon_mass\"].array()\n",
    "    events[dataset][\"muon_iso\"] = dataset_events[\"Muon_miniIsoId\"].array()\n",
    "    events[dataset][\"muon_tightId\"] = dataset_events[\"Muon_tightId\"].array()\n",
    "\n",
    "    events[dataset][\"fatjet_tag\"] = dataset_events[\"FatJet_particleNet_TvsQCD\"].array()\n",
    "    events[dataset][\"fatjet_pt\"] = dataset_events[\"FatJet_pt\"].array()\n",
    "    events[dataset][\"fatjet_eta\"] = dataset_events[\"FatJet_eta\"].array()\n",
    "    events[dataset][\"fatjet_phi\"] = dataset_events[\"FatJet_phi\"].array()\n",
    "    events[dataset][\"fatjet_mass\"] = dataset_events[\"FatJet_mass\"].array()\n",
    "\n",
    "    events[dataset][\"jet_btag\"] = dataset_events[\"Jet_btagDeepB\"].array()\n",
    "    events[dataset][\"jet_jetid\"] = dataset_events[\"Jet_jetId\"].array()\n",
    "    events[dataset][\"jet_pt\"] = dataset_events[\"Jet_pt\"].array()\n",
    "    events[dataset][\"jet_eta\"] = dataset_events[\"Jet_eta\"].array()\n",
    "    events[dataset][\"jet_phi\"] = dataset_events[\"Jet_phi\"].array()\n",
    "    events[dataset][\"jet_mass\"] = dataset_events[\"Jet_mass\"].array()\n",
    "\n",
    "    events[dataset][\"met_pt\"] = dataset_events[\"PuppiMET_pt\"].array()\n",
    "    events[dataset][\"met_eta\"] = 0 * dataset_events[\"PuppiMET_pt\"].array()  # Fix this to be 0 !!\n",
    "    events[dataset][\"met_phi\"] = dataset_events[\"PuppiMET_phi\"].array()\n",
    "    #==================== Event variables =========================\n",
    "    events[dataset][\"ht_lep\"] = events[dataset][\"muon_pt\"] + events[dataset][\"met_pt\"]\n",
    "\n",
    "    # meta\n",
    "    if dataset != \"data\":\n",
    "        events[dataset][\"weights\"] = dataset_events[\"genWeight\"].array()\n",
    "        events[dataset][\"nTrueInt\"] = dataset_events[\"Pileup_nTrueInt\"].array()\n",
    "    else:\n",
    "        events[dataset][\"weights\"] = np.ones(events[dataset][\"nevents\"])\n",
    "        events[dataset][\"nTrueInt\"] = np.ones(events[dataset][\"nevents\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define object and event selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/cms-opendata-workshop/workshop2024-lesson-event-selection/blob/main/instructors/dpoa_workshop_utilities.py\n",
    "def build_lumi_mask(lumifile, tree, verbose=False):\n",
    "\n",
    "    # lumifile should be the name/path of the file\n",
    "    good_luminosity_sections = ak.from_json(open(lumifile, 'rb'))\n",
    "    # Pull out the good runs as integers\n",
    "    good_runs = np.array(good_luminosity_sections.fields).astype(int)\n",
    "\n",
    "    # Get the good blocks as an awkward array\n",
    "    # First loop over to get them as a list\n",
    "    all_good_blocks = []\n",
    "    for field in good_luminosity_sections.fields:\n",
    "        all_good_blocks.append(good_luminosity_sections[field])\n",
    "\n",
    "    # Turn the list into an awkward array\n",
    "    all_good_blocks = ak.Array(all_good_blocks)\n",
    "\n",
    "    # Get the runs and luminosity blocks from the tree\n",
    "    run = tree['run'].array()\n",
    "    lumiBlock = tree['luminosityBlock'].array()\n",
    "\n",
    "\n",
    "    # ChatGPT helped me with this part!\n",
    "    # Find index of values in arr2 if those values appear in arr1\n",
    "    def find_indices(arr1, arr2):\n",
    "        arr1_np = np.asarray(ak.to_numpy(arr1))\n",
    "        arr2_np = np.asarray(ak.to_numpy(arr2))\n",
    "\n",
    "        # Sort arr1 and track original indices\n",
    "        sorter = np.argsort(arr1_np)\n",
    "        sorted_arr1 = arr1_np[sorter]\n",
    "\n",
    "        # Search positions\n",
    "        pos = np.searchsorted(sorted_arr1, arr2_np)\n",
    "\n",
    "        # Check if arr2 values actually exist in arr1\n",
    "        valid = (pos < len(arr1_np)) & (sorted_arr1[pos] == arr2_np)\n",
    "\n",
    "        # Prepare result\n",
    "        out = np.full(len(arr2_np), -1, dtype=int)\n",
    "        out[valid] = sorter[pos[valid]]\n",
    "        return ak.Array(out)\n",
    "\n",
    "    # Get the indices that say where the good runs are in the lumi file\n",
    "    # for the runs that appear in the tree\n",
    "    good_runs_indices = find_indices(good_runs, run)\n",
    "\n",
    "    # For each event, calculate the difference between the luminosity block for that event\n",
    "    # and the good luminosity blocks for that run for that event\n",
    "    diff = lumiBlock - all_good_blocks[good_runs_indices]\n",
    "\n",
    "\n",
    "    # If the lumi block appears between any of those good block numbers,\n",
    "    # then one difference will be positive and the other will be negative\n",
    "    #\n",
    "    # If it it outside of the range, both differences will be positive or\n",
    "    # both negative.\n",
    "    #\n",
    "    # The product will be negagive if the lumi block is in the range\n",
    "    # and positive if it is not in the range\n",
    "    prod_diff = ak.prod(diff, axis=2)\n",
    "    mask = ak.any(prod_diff<=0, axis=1)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tau32 = fatjet_tau3/fatjet_tau2\n",
    "#cut_fatjet = (tau32>0.67) & (fatjet_eta>-2.4) & (fatjet_eta<2.4) & (fatjet_mSD>105) & (fatjet_mSD<220)\n",
    "for dataset, data in events.items():\n",
    "\n",
    "    events[dataset][\"obj_sel\"] = {\n",
    "        \"fatjets\": (data[\"fatjet_pt\"] > 500) & (data[\"fatjet_tag\"] > 0.5),\n",
    "        \"muons\": (data[\"muon_pt\"] > 55) & (data[\"muon_eta\"] > -2.4) & (data[\"muon_eta\"] < 2.4) &\n",
    "                (data[\"muon_tightId\"] == True) & (data[\"muon_iso\"] > 1) & (data[\"ht_lep\"] > 150),\n",
    "        \"jets\": (data[\"jet_btag\"] > 0.5) & (data[\"jet_jetid\"] >= 4)\n",
    "    }\n",
    "    # Event cut\n",
    "    events[dataset][\"event_sel\"] = {\n",
    "        \"met\": (data[\"met_pt\"] > 50),\n",
    "        \"nmuons\": (ak.sum(data[\"obj_sel\"][\"muons\"], axis=1) == 1),\n",
    "        \"trigger\": (data[\"events\"][\"HLT_TkMu50\"].array()),\n",
    "        \"btag\": (ak.sum(data[\"obj_sel\"][\"jets\"], axis=1) > 0),\n",
    "        \"ntop\": (ak.sum(data[\"obj_sel\"][\"fatjets\"], axis=1) == 1)\n",
    "    }\n",
    "\n",
    "    full_event_sel =  ((events[dataset][\"event_sel\"][\"trigger\"]) &\n",
    "        (events[dataset][\"event_sel\"][\"nmuons\"]) &\n",
    "        (events[dataset][\"event_sel\"][\"met\"]) &\n",
    "        (events[dataset][\"event_sel\"][\"ntop\"]) &\n",
    "        (events[dataset][\"event_sel\"][\"btag\"]))\n",
    "\n",
    "    if dataset == \"data\":\n",
    "        mask_lumi = build_lumi_mask('Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt', events[dataset][\"events\"], verbose=False)\n",
    "        full_event_sel = full_event_sel & (mask_lumi)\n",
    "\n",
    "    events[dataset][\"event_sel\"][\"full\"] = full_event_sel\n",
    "\n",
    "    full_event_sel = events[dataset][\"event_sel\"][\"full\"]\n",
    "\n",
    "    print(f\"Fraction of {dataset} events left: \", ak.sum(events[dataset][\"event_sel\"][\"full\"])/nevents, \"Total left: \",  ak.sum(events[dataset][\"event_sel\"][\"full\"]))\n",
    "\n",
    "    fatjets = ak.zip(\n",
    "        {\n",
    "            \"pt\": data[\"fatjet_pt\"][full_event_sel][data[\"obj_sel\"][\"fatjets\"][full_event_sel]],\n",
    "            \"eta\": data[\"fatjet_eta\"][full_event_sel][data[\"obj_sel\"][\"fatjets\"][full_event_sel]],\n",
    "            \"phi\": data[\"fatjet_phi\"][full_event_sel][data[\"obj_sel\"][\"fatjets\"][full_event_sel]],\n",
    "            \"mass\": data[\"fatjet_mass\"][full_event_sel][data[\"obj_sel\"][\"fatjets\"][full_event_sel]],\n",
    "        },\n",
    "        with_name=\"Momentum4D\",\n",
    "    )\n",
    "    events[dataset][\"fatjets\"] = fatjets\n",
    "\n",
    "    muons = ak.zip(\n",
    "        {\n",
    "            \"pt\": data[\"muon_pt\"][full_event_sel][data[\"obj_sel\"][\"muons\"][full_event_sel]],\n",
    "            \"eta\": data[\"muon_eta\"][full_event_sel][data[\"obj_sel\"][\"muons\"][full_event_sel]],\n",
    "            \"phi\": data[\"muon_phi\"][full_event_sel][data[\"obj_sel\"][\"muons\"][full_event_sel]],\n",
    "            \"mass\": data[\"muon_mass\"][full_event_sel][data[\"obj_sel\"][\"muons\"][full_event_sel]],\n",
    "        },\n",
    "        with_name=\"Momentum4D\",\n",
    "    )\n",
    "    print(muons)\n",
    "    events[dataset][\"muons\"] = muons\n",
    "\n",
    "    jets = ak.zip(\n",
    "        {\n",
    "            \"pt\": data[\"jet_pt\"][full_event_sel][data[\"obj_sel\"][\"jets\"][full_event_sel]][:, 0],\n",
    "            \"eta\": data[\"jet_eta\"][full_event_sel][data[\"obj_sel\"][\"jets\"][full_event_sel]][:, 0],\n",
    "            \"phi\": data[\"jet_phi\"][full_event_sel][data[\"obj_sel\"][\"jets\"][full_event_sel]][:, 0],\n",
    "            \"mass\": data[\"jet_mass\"][full_event_sel][data[\"obj_sel\"][\"jets\"][full_event_sel]][:, 0],\n",
    "        },\n",
    "        with_name=\"Momentum4D\",\n",
    "    )\n",
    "    events[dataset][\"jets\"] = jets\n",
    "\n",
    "    met = ak.zip(\n",
    "        {\n",
    "            \"pt\": data[\"met_pt\"][data[\"event_sel\"][\"full\"]],\n",
    "            \"eta\": data[\"met_eta\"][data[\"event_sel\"][\"full\"]],\n",
    "            \"phi\": data[\"met_phi\"][data[\"event_sel\"][\"full\"]],\n",
    "            \"mass\": 0,\n",
    "        },\n",
    "        with_name=\"Momentum4D\",\n",
    "    )\n",
    "    events[dataset][\"met\"] = met\n",
    "\n",
    "    p4mu,p4fj,p4j,p4met = ak.unzip(ak.cartesian([muons, fatjets, jets, met]))\n",
    "    p4tot = p4mu + p4fj + p4j + p4met\n",
    "    events[dataset][\"p4tot\"] = p4tot\n",
    "    events[dataset]['mtt'] = ak.flatten(p4tot.mass)\n",
    "    events[dataset]['mu_pt'] = ak.flatten(p4mu.pt)\n",
    "    events[dataset]['mu_abseta'] = np.abs(ak.flatten(p4mu.eta))\n",
    "    events[dataset]['weight'] = data[\"weights\"][full_event_sel]\n",
    "    events[dataset]['pileup'] = data[\"nTrueInt\"][full_event_sel]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, data in events.items():\n",
    "    print(f\"Plotting {dataset} dataset\")\n",
    "    p4tot = data[\"p4tot\"]\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.title(f\"{dataset} - Fatjet mass\")\n",
    "    plt.hist(ak.flatten(p4tot.mass),bins=50, range=(0,7000));\n",
    "    plt.xlabel(\"Fatjet mass [GeV]\")\n",
    "    plt.ylabel(\"Number of events\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dataset, data in events.items():\n",
    "    to_write = {}\n",
    "    to_write['mtt'] = data['mtt']\n",
    "    to_write['mu_pt'] = data['mu_pt']\n",
    "    to_write['mu_abseta'] = data['mu_abseta']\n",
    "\n",
    "    to_write['weight'] = data[\"weight\"]\n",
    "    to_write['pileup'] = data[\"pileup\"]\n",
    "    df = pd.DataFrame.from_dict(to_write)\n",
    "    outfilename = f\"output_{dataset}_{filename.split('/')[-1].split('.')[0]}.csv\"\n",
    "    df.to_csv(outfilename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open(\"puWeights.json.gz\",'rt') as file:\n",
    "    data = file.read().strip()\n",
    "    pu_corr_evaluator = correctionlib._core.CorrectionSet.from_string(data)\n",
    "\n",
    "with gzip.open(\"./muon_Z.json.gz\",'rt') as file:\n",
    "    data = file.read().strip()\n",
    "    muo_corr_evaluator = correctionlib._core.CorrectionSet.from_string(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = {}\n",
    "datadict['signal'] = np.genfromtxt('output_signal_22BAB5D2-9E3F-E440-AB30-AE6DBFDF6C83.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['tt_semilep'] = np.genfromtxt('output_tt_semilep_08FCB2ED-176B-064B-85AB-37B898773B98.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['tt_had'] = np.genfromtxt('output_tt_had_009086DB-1E42-7545-9A35-1433EC89D04B.csv', delimiter=',', names=True, dtype=float)\n",
    "datadict['data'] = np.genfromtxt('output_data_0DEE1709-0416-F24B-ACB2-C68997CB6465.csv', delimiter=',', names=True, dtype=float)\n",
    "\n",
    "hists = {}\n",
    "for sample in datadict.keys():\n",
    "    print(sample)\n",
    "    hists[sample] = {\n",
    "        \"pu\": datadict[sample]['pileup'],\n",
    "        \"mu_pt\": datadict[sample]['mu_pt'],\n",
    "        \"mu_abseta\": datadict[sample]['mu_abseta'],\n",
    "        \"genWeight\": datadict[sample]['weight']/np.abs(datadict[sample]['weight']),\n",
    "        \"mtt\": datadict[sample]['mtt'],\n",
    "        \"pu_weight\": [pu_corr_evaluator[\"Collisions16_UltraLegacy_goldenJSON\"].evaluate(n,\"nominal\") for n in datadict[sample]['pileup']],\n",
    "        \"pu_weight_up\": [pu_corr_evaluator[\"Collisions16_UltraLegacy_goldenJSON\"].evaluate(n,\"up\") for n in datadict[sample]['pileup']],\n",
    "        \"pu_weight_dn\": [pu_corr_evaluator[\"Collisions16_UltraLegacy_goldenJSON\"].evaluate(n,\"down\") for n in datadict[sample]['pileup']],\n",
    "        \"leadmuon_sf\": [muo_corr_evaluator[\"NUM_TightID_DEN_TrackerMuons\"].evaluate(eta,pt,\"nominal\") for pt,eta in zip(datadict[sample]['mu_pt'],datadict[sample]['mu_abseta'])],\n",
    "        \"leadmuon_sf_up\": [muo_corr_evaluator[\"NUM_TightID_DEN_TrackerMuons\"].evaluate(eta,pt,\"systup\") for pt,eta in zip(datadict[sample]['mu_pt'],datadict[sample]['mu_abseta'])],\n",
    "        \"leadmuon_sf_dn\": [muo_corr_evaluator[\"NUM_TightID_DEN_TrackerMuons\"].evaluate(eta,pt,\"systdown\") for pt,eta in zip(datadict[sample]['mu_pt'],datadict[sample]['mu_abseta'])],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hists['signal']['mtt'],\n",
    "         bins=50,range=(0,3000),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['pu_weight'],\n",
    "         histtype=\"step\",color=\"k\",label=\"nominal\")\n",
    "plt.hist(hists['signal']['mtt'],\n",
    "         bins=50,range=(0,3000),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['pu_weight_up'],\n",
    "         histtype=\"step\",color=\"r\",label=\"up\")\n",
    "plt.hist(hists['signal']['mtt'],\n",
    "         bins=50,range=(0,3000),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['pu_weight_dn'],\n",
    "         histtype=\"step\",color=\"b\",label=\"down\")\n",
    "plt.legend()\n",
    "plt.xlabel('ttbar mass (GeV)')\n",
    "plt.ylabel('events / 60 GeV')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(hists['signal']['pu'],\n",
    "         bins=50,range=(0,50),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['pu_weight'],\n",
    "         histtype=\"step\",color=\"k\",label=\"nominal\")\n",
    "plt.hist(hists['signal']['pu'],\n",
    "         bins=50,range=(0,50),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['pu_weight_up'],\n",
    "         histtype=\"step\",color=\"r\",label=\"up\")\n",
    "plt.hist(hists['signal']['pu'],\n",
    "         bins=50,range=(0,50),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['pu_weight_dn'],\n",
    "         histtype=\"step\",color=\"b\",label=\"down\")\n",
    "plt.legend()\n",
    "plt.xlabel('N true interactions')\n",
    "plt.ylabel('events')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(hists['signal']['mu_pt'],\n",
    "         bins=50,range=(0,50),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['leadmuon_sf'],\n",
    "         histtype=\"step\",color=\"k\",label=\"nominal\")\n",
    "plt.hist(hists['signal']['pu'],\n",
    "         bins=50,range=(0,50),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['leadmuon_sf_up'],\n",
    "         histtype=\"step\",color=\"r\",label=\"up\")\n",
    "plt.hist(hists['signal']['pu'],\n",
    "         bins=50,range=(0,50),\n",
    "         weights=hists['signal']['genWeight']*hists['signal']['leadmuon_sf_dn'],\n",
    "         histtype=\"step\",color=\"b\",label=\"down\")\n",
    "plt.legend()\n",
    "plt.xlabel('Muon pT (GeV)')\n",
    "plt.ylabel('events')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('hists_for_ROOT.p','wb') as f:\n",
    "    pickle.dump(hists,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hist\n",
    "\n",
    "info = {'signal': {\"xsec\": 1.0, \"binning\": (0, 3000, 50)},\n",
    "        'tt_semilep': {\"xsec\": 831.76*0.438, \"binning\": (0, 3000, 50)},\n",
    "        'tt_had': {\"xsec\": 831.76*0.457, \"binning\": (0, 3000, 50)},\n",
    "        }\n",
    "\n",
    "with open('hists_for_ROOT.p','rb') as f:\n",
    "    hists = pickle.load(f)\n",
    "\n",
    "\n",
    "SYSTEMATICS = {\n",
    "    \"leadmuon_sf\": {\n",
    "        \"up\": \"leadmuon_sf_up\",\n",
    "        \"dn\": \"leadmuon_sf_dn\",\n",
    "        \"nominal\": \"leadmuon_sf\",\n",
    "        \"type\": \"weight\",\n",
    "    },\n",
    "    \"pu_weight\": {\n",
    "        \"up\": \"pu_weight_up\",\n",
    "        \"dn\": \"pu_weight_dn\",\n",
    "        \"nominal\": \"pu_weight\",\n",
    "        \"type\": \"weight\",\n",
    "    },\n",
    "}\n",
    "\n",
    "observable_name = \"mtt\"\n",
    "ndhist = ( hist.Hist.new.Reg(50, 0, 3000, name=\"observable\", label=f\"{observable_name} [GeV]\")\n",
    "          .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "          .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "          .Weight()\n",
    "        )\n",
    "\n",
    "for process, process_info in hists.items():\n",
    "\n",
    "    print(f\"Processing {process} dataset\")\n",
    "    # load observable data\n",
    "    observable = process_info[observable_name]\n",
    "    # Handle weights\n",
    "    print(process)\n",
    "    if process == \"data\":\n",
    "        weight = 1.0\n",
    "    else:\n",
    "        genweight = process_info['genWeight']\n",
    "        n_gen = sum(genweight)\n",
    "        lumi_weight = 16400*info[process][\"xsec\"]*process_info['genWeight']/n_gen\n",
    "        pu_weight = process_info['pu_weight']\n",
    "        mu_id_sf = process_info['leadmuon_sf']\n",
    "        weight = genweight * lumi_weight * pu_weight * mu_id_sf\n",
    "\n",
    "    # Fill nominal histogram\n",
    "    ndhist.fill(\n",
    "                observable=observable, process=process,\n",
    "                variation=\"nominal\", weight=weight,\n",
    "            )\n",
    "    if process == \"data\":\n",
    "            continue\n",
    "    for variation, variation_info in SYSTEMATICS.items():\n",
    "        for direction in [\"up\", \"dn\"]:\n",
    "\n",
    "            syst_var_name = f\"{variation}_{direction}\"\n",
    "            var_weight = weight\n",
    "            if variation_info[\"type\"] == \"weight\":\n",
    "                var_weight *= np.asarray(process_info[variation_info[direction]])/np.asarray(process_info[variation_info[\"nominal\"]])\n",
    "\n",
    "            ndhist.fill(\n",
    "                        observable=observable, process=process,\n",
    "                        variation=syst_var_name, weight=var_weight,\n",
    "                    )\n",
    "\n",
    "print(ndhist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "\n",
    "hists_dict = {\"channel\": ndhist}\n",
    "def save_histograms(hist_dict, filename, add_offset=False):\n",
    "    with uproot.recreate(filename) as f:\n",
    "        # save all available histograms to disk\n",
    "        for channel, histogram in hist_dict.items():\n",
    "            # optionally add minimal offset to avoid completely empty bins\n",
    "            # (useful for the ML validation variables that would need binning adjustment\n",
    "            # to avoid those)\n",
    "            if add_offset:\n",
    "                histogram += 1e-6\n",
    "                # reference count for empty histogram with floating point math tolerance\n",
    "                empty_hist_yield = histogram.axes[0].size*(1e-6)*1.01\n",
    "            else:\n",
    "                empty_hist_yield = 0\n",
    "\n",
    "            for sample in histogram.axes[1]:\n",
    "                for variation in histogram[:, sample, :].axes[1]:\n",
    "                    variation_string = \"\" if variation == \"nominal\" else f\"_{variation}\"\n",
    "                    current_1d_hist = histogram[:, sample, variation]\n",
    "\n",
    "                    if sum(current_1d_hist.values()) > empty_hist_yield:\n",
    "                        # only save histograms containing events\n",
    "                        f[f\"{channel}_{sample}{variation_string}\"] = current_1d_hist\n",
    "\n",
    "            # Need to build some notion of pseudodata here\n",
    "\n",
    "save_histograms(hists_dict, \"test_hists.root\", add_offset=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
